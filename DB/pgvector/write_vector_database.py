import psycopg2
from sentence_transformers import SentenceTransformer
import json
import os
import time

# PostgreSQL baÄŸlantÄ±sÄ±
def connect_db():
    try:
        conn = psycopg2.connect(
            host="localhost",
            database="vector_db",
            user="postgres",
            password="yeni_sifre",
            port="5432"
        )
        return conn
    except Exception as e:
        print(f"BaÄŸlantÄ± hatasÄ±: {e}")
        return None

# VeritabanÄ± ve tablo oluÅŸtur
def create_table(conn):
    cursor = conn.cursor()
    try:
        # pgvector extension'Ä±nÄ± etkinleÅŸtir
        cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
        
        # Eski tabloyu sil (temiz baÅŸlangÄ±Ã§ iÃ§in)
        cursor.execute("DROP TABLE IF EXISTS documents;")
        
        # Tablo oluÅŸtur
        cursor.execute("""
            CREATE TABLE documents (
                id BIGSERIAL PRIMARY KEY,
                chunk_id INT,
                doc_id INT,
                filename VARCHAR(255),
                filepath TEXT,
                metin TEXT,
                embedding vector(384)
            );
        """)
        
        # HNSW index oluÅŸtur (cosine distance iÃ§in)
        cursor.execute("""
            CREATE INDEX ON documents 
            USING hnsw (embedding vector_cosine_ops)
            WITH (m = 16, ef_construction = 64);
        """)
        
        conn.commit()
        print("âœ“ Tablo ve index baÅŸarÄ±yla oluÅŸturuldu")
    except Exception as e:
        print(f"âœ— Tablo oluÅŸturma hatasÄ±: {e}")
        conn.rollback()
    finally:
        cursor.close()

# Metni parÃ§alara bÃ¶l
def metni_parcala(metin, chunk_size=500, overlap=100):
    """Metni Ã§akÄ±ÅŸmalÄ± parÃ§alara bÃ¶l"""
    parcalar = []
    kelimeler = metin.split()
    
    for i in range(0, len(kelimeler), chunk_size - overlap):
        parca = ' '.join(kelimeler[i:i + chunk_size])
        if len(parca.strip()) > 50:
            parcalar.append(parca.strip())
    
    return parcalar

# Veri ekle
def insert_documents(conn, veriler):
    cursor = conn.cursor()
    model = SentenceTransformer("all-MiniLM-L6-v2")
    
    batch_size = 100
    total_inserted = 0
    
    for i in range(0, len(veriler), batch_size):
        batch_veriler = veriler[i:i+batch_size]
        batch_metinler = [v["metin"] for v in batch_veriler]
        
        # Embedding oluÅŸtur
        embeddings = model.encode(batch_metinler)
        
        try:
            for j, (veri, embedding) in enumerate(zip(batch_veriler, embeddings)):
                embedding_str = '[' + ','.join(f'{x:.8f}' for x in embedding.tolist()) + ']'
                
                cursor.execute("""
                    INSERT INTO documents (chunk_id, doc_id, filename, filepath, metin, embedding)
                    VALUES (%s, %s, %s, %s, %s, %s::vector)
                """, (
                    veri["chunk_id"], 
                    veri["doc_id"], 
                    veri["filename"], 
                    veri["filepath"], 
                    veri["metin"], 
                    embedding_str
                ))
            
            conn.commit()
            total_inserted += len(batch_veriler)
            print(f"âœ“ {total_inserted}/{len(veriler)} kayÄ±t eklendi...")
        except Exception as e:
            print(f"âœ— Veri ekleme hatasÄ±: {e}")
            conn.rollback()
    
    cursor.close()
    print(f"\nâœ“ Toplam {total_inserted} kayÄ±t baÅŸarÄ±yla eklendi")

# Ana fonksiyon
def main():
    # JSON dosyasÄ±nÄ± oku
    json_dosya_yolu = '/home/ugo/Documents/Python/bitirememe projesi/metin_dosyasi.json'
    
    if not os.path.exists(json_dosya_yolu):
        print(f"âœ— HATA: {json_dosya_yolu} dosyasÄ± bulunamadÄ±!")
        return
    
    with open(json_dosya_yolu, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # DÃ¶kÃ¼manlarÄ± Ã§Ä±kar
    documents = data.get("documents", [])
    print(f"âœ“ Toplam {len(documents)} adet dÃ¶kÃ¼man bulundu")
    
    # TÃ¼m parÃ§alarÄ± topla
    tum_veriler = []
    
    for doc in documents:
        doc_id = doc.get("id", 0)
        filename = doc.get("filename", "")
        filepath = doc.get("filepath", "")
        full_text = doc.get("full_text", "")
        
        if not full_text or len(full_text.strip()) < 50:
            print(f"âš  Atlanan dÃ¶kÃ¼man (boÅŸ veya Ã§ok kÄ±sa): {filename}")
            continue
        
        # Metni parÃ§ala
        parcalar = metni_parcala(full_text, chunk_size=300, overlap=50)
        
        for chunk_idx, parca in enumerate(parcalar):
            tum_veriler.append({
                "metin": parca,
                "chunk_id": chunk_idx,
                "doc_id": doc_id,
                "filename": filename,
                "filepath": filepath
            })
    
    print(f"âœ“ Toplam {len(tum_veriler)} adet metin parÃ§asÄ± oluÅŸturuldu\n")
    
    # VeritabanÄ±na baÄŸlan
    conn = connect_db()
    if not conn:
        return
    
    try:
        # Tablo oluÅŸtur
        create_table(conn)
        
        # Veri ekle
        print("ðŸ“ Veri ekleniyor...\n")
        start_time = time.time()
        insert_documents(conn, tum_veriler)
        insert_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print(f"âœ“ {len(tum_veriler)} adet veri baÅŸarÄ±yla pgvector'e kaydedildi!")
        print(f"âœ“ Toplam ekleme zamanÄ±: {insert_time:.2f}s")
        print(f"{'='*60}")
        
        # KayÄ±t sayÄ±sÄ±nÄ± kontrol et
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM documents;")
        count = cursor.fetchone()[0]
        cursor.close()
        
        print(f"\nðŸ“Š Tablo Bilgisi:")
        print(f"   Tablo adÄ±: documents")
        print(f"   Toplam kayÄ±t: {count}")
        print(f"âœ“ Ä°ÅŸlem tamamlandÄ±")
            
    finally:
        conn.close()
        print("âœ“ VeritabanÄ± baÄŸlantÄ±sÄ± kapatÄ±ldÄ±")

if __name__ == "__main__":
    main()
